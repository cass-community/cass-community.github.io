---
title: Whole Device Modeling of Magnetically Confined Fusion Plasma
date: 2025-01-24
teaser: 2025-01-wdmapp/wdmapp-coupled-turbulence.jpg
software_mentioned:
  - AMReX
  - MAGMA
  - libEnsemble
  - xSDK
  - Spack
  - E4S
  - HDF5
  - ADIOS
  - ParaView
  - ParaView Catalyst
  - Ascent
  - VisIt
  - VTK-m
  - MPICH
  - Open MPI
  - Darshan
  - HPCToolkit
  - TAU
  - PAPI
  - Dyninst
#
cass_members:
  - FASTMath
  - PESO
  - RAPIDS
  - S4PST
  - STEP
#
# The excerpt is used on the main Impacts page.
# The heading at the beginning of the body messes up the automatic excerpting process.
#  
excerpt: Magnetically confined fusion plasmas are being designed within the International Tokamak Experimental Reactor (ITER) and other projects that will operate in physics regimes only recently achieved through experiment. Modeling and simulation activities are required to design and optimize these new facilities. The fusion community is developing an approach to whole device modeling that will provide predictive numerical simulations of the physics required for magnetically confined fusion plasmas to enable design optimization and fill in the experimental gaps for ITER and future fusion devices.
---
## The science

{% capture img %}{% include hl-image-path image="2025-01-wdmapp/wdmapp-coupled-turbulence.jpg" %}{% endcapture %}
{% include figure class="align-right" width="50%" popup=true image_path=img alt="Image showing plasma turbulence in a tokamak reactor" caption="The first coupled simulation of turbulence in a tokamak device. Visualization courtesy of D. Pugmire (ORNL)." %}

Magnetically confined fusion plasmas are being designed within the International Tokamak Experimental Reactor (ITER) and other projects that will operate in physics regimes only recently achieved through experiment. Modeling and simulation activities are required to design and optimize these new facilities. The fusion community is developing an approach to whole device modeling that will provide predictive numerical simulations of the physics required for magnetically confined fusion plasmas to enable design optimization and fill in the experimental gaps for ITER and future fusion devices.

## The enabling software

Coupled magnetic fusion simulation codes rely on a wide range of other software to deliver  scientific capabilities. The following highlights software from the CASS scientific software ecosystem used by the WDMApp project, which was part of the DOE Exascale Computing Project.

### Mathematical libraries (FASTMath)

WDMApp build options bring in the xSDK in order to use many of the FASTMath libraries.  Math libraries in the CASS ecosystem that are used by WDMApp codes, either by default or as variant options, include **PETSc**, **hypre**, **SuperLU**, **Trilinos**, **AMReX**, **MFEM**, **SUNDIALS**, **Kokkos Kernels**, **Strumpack**, **libEnsemble**, **TAO**, **DIY**, **Ginkgo**, **libCEED**, **MAGMA**, and **PUMI**.

### Software ecosystem and delivery (PESO)

{% capture img %}{% include hl-image-path image="2025-01-wdmapp/e4s-build-cache.png" %}{% endcapture %}
{% include figure class="align-right" width="50%" popup=true image_path=img alt="Listing of packages in an E4S build cache" caption="A brief listing from the E4S build cache for Spack.  All available software builds for the various architectures can be found at: <https://oaciss.uoregon.edu/e4s/inventory.html>" %}

Leveraging **Spack** and **E4S** binaries is one of the options for WDMApp builds.  Building a code requires converting the source code into machine-readable instructions.  For a simulation as complex as WDMApp, this means bringing in many software libraries -- a process that can take a significant amount of time.  Using a prebuilt binary cache means that many of the necessary libraries have already been compiled.  Thus the E4S approach allows developers and users to take use the build caches, pre-built containers, and installations at a variety of HPC facilities in order to speed up the compilation process.  WDMApp has achieved significant speedups in build time via Spack and E4S.

### Data and visualization (RAPIDS)

{% capture img %}{% include hl-image-path image="2025-01-wdmapp/PoincarePlots.jpg" %}{% endcapture %}
{% include figure class="align-right" width="50%" popup=true image_path=img alt="Multiple Poincare plot images" caption="Poincare plots generated from eight different time steps from a run of WDMApp’s XGC code on Frontier.  Attribution: ORNL: David Pugmire, Kenneth Moreland, Jong Choi, Scott Klasky. PPPL: Chong-Seock Chang, Seung-Hoe Ku, Robert Hager." %}

WDMApp leverages **ADIOS** for code coupling and fast I/O.  **HDF5** and **PnetCDF** are available as optional I/O technologies.  

**zfp** compression is available through a Spack build option or can be used through a plugin to ADIOS.

WDMApp leveraged **VTK-m** to develop Poincare maps in its XGC code to plot the magnetic fields in the tokamak, allowing scientists to see how the magnetic field changes over time.  Use of VTK-m significantly speeded up the Poincare calculations.  

### Programming models and runtimes (S4PST)

WDMApp targets heterogeneous HPC architectures using **Kokkos** as its performance portability model.  Additionally, Kokkos is an optional backend for VTK-m for GPU performance and can be leveraged by WDMApp simulations for efficient visualization tasks on the GPU.  

WDMApp allows multiple options for **MPI** via build flags.  **LLVM/Clang**, **OpenACC** and **OpenMP** are available via WDMApp’s configuration options.  

**Open MPI**, **MPICH**, **OpenMP’s V&V suite**, and **OpenACC’s V&V suite** are part of the S4PST CASS member organization, as is Kokkos.

### Development tools (STEP)

**TAU**, **HPCToolkit**, **PAPI** and **Darshan** are available through the WDMApp Spack configuration file. These are available in the suite of development tools in the STEP CASS member organization and are designed to be used by scientific simulation codes for parallel high-performance application debugging and performance profiling. 

## Additional resources

**WDMApp citation:** 

https://doi.org/10.2312/visgap.20241120<br>
https://arxiv.org/pdf/2311.01288 

*Authors: Terry Turton, David Bernholdt and Lois Curfman McInnes*<br>
*Acknowledgment: Amitava Bhattacharjee and the WDMApp team*