---
title: LAMMPS & Kokkos: Using Kokkos in LAMMPS to enable molecular dynamics simulations across scales of accuracy, length and time  
date: 2025-09-11
#
#teaser: 
#
authors:
  - Terece L. Turton
  - David E. Bernholdt
  - Lois Curfman McInnes
  - Michael Heroux
acknowledgment: the LAMMPS team
#
# The excerpt is used on the main Impacts page.
# The heading at the beginning of the body messes up the automatic excerpting process.
#  
excerpt: LAMMPS is a classical molecular dynamics code widely used across the DOE complex. Leveraging Kokkos, LAMMPs is able to run on a wide variety of high performance computing resources.  
#
software_mentioned:
  - Kokkos
---

## The science

[LAMMPS](https://www.lammps.org/)  (Large-scale Atomic/Molecular Massively Parallel Simulator) is a classical molecular dynamics simulation that is important for materials modeling.  LAMMPS is a critical code within the DOE Office of Science (SC), funded over many years, in part by the DOE SC Advanced Scientific Computer Research (ASCR) and  Biological and Environmental Research (BER) programs.  Widely used  across many physics domains, LAMMPS can be used for solid-state materials such as metals and semiconductors, or soft matter such as biomolecules or polymers.  It can be used to model systems at the atomic, meso, or continuum scale.

Additionally, LAMMPS is designed to run on single processors or in massively parallel modes on high performance computing resources.  To reach the most extreme scales, LAMMPS requires an accelerator package, such as the one that utilizes the Kokkos performance portability library.  

## The enabling software: Kokkos

Kokkos is a C++ library that provides an abstraction layer allowing a code to easily run on different computer architectures, including CPUs and GPUs from various vendors.  The {% include sw-link-mention.html product="Kokkos" %} library also provides data abstractions to change the memory layout of data structures such as 2D and 3D arrays to optimize performance on different hardware.

LAMMPS documentation includes extensive details on using Kokkos as the accelerator package to improve LAMMPS performance.  

Long term funding via both the DOE Office of Science and the DOE National Nuclear Security Adminstration has contributed to the success of Kokkos and thus its clients such as LAMMPS.  

{% capture img %}{% include hl-image-path image="2025-09-lammps/lammps-kokkos-documentation.png" %}{% endcapture %}
{% include figure class="align-right" width="50%" popup=true image_path=img alt="Image showing Kokkos usage in LAMMPS documentation" 
caption="A screenshot from the LAMMPS documentation on how to include and use Kokkos within LAMMPS." %}


 To take advantage of GPU accelerators, LAMMPS provides multiple different accelerator packages (per the LAMMPS documentation, as of April 2025).

| Hardware            | Acceleration Support Package           |
| --------------------| -------------------------------------- |
| Many-core CPUS      | INTEL, KOKKOS, OPENMP, OPt packages    |
| GPUS                | GPU, KOKKOS packages                   |
| Intel Phi/AVX       | INTEL, KOKKOS packages                 |

Kokkos supports LAMMPS by targeting the full range of possible backend programming models used by LAMMPS: CUDA, HIP, SYCL, HPX, OpenMP and C++ threads, allowing applications such as LAMMPS to run on all major HPC platforms.  

Using Kokkos, LAMMPS is able to run on a wide variety of architectures.  The billion atom simulation (Ngyen-Cong, et al. 2021) shown in the second figure uses a machine learned model for the descriptions of interatomic bonding.  Versions of this simulation have been run, for example, on a small local cluster of NVidia H100 GPUs, on Oak Ridge National Laboratory's (ORNL) Summit V100 GPUs and on ORNL's Frontier exascale supercomputer using AMD MI250X GPUs.  

{% capture img %}{% include hl-image-path image="2025-09-lammps/lammps-ecp-fusion-graphic.png" %}{% endcapture %}
{% include figure class="align-right" width="50%" popup=true image_path=img alt="Image showing a LAMMPS and Kokkos use case.  This is a close-up with He, W and ZrC molecules."
caption="An image from a billion atom LAMMPS molecular dynamics simulation.  Utilizing Kokkos, this simulation was run on ORNL's Frontier supercomputing, using more than 8000 of the machine's MI250X GPUs. Images curtesy of Mitchell Wood (SNL) and the LAMMPS team." %}

The LAMMPS/Kokkos integration is an example of the benefits of collaboration across the Department of Energy. During the Exascale Computing Project (ECP), LAMMPS was funded through the Office of Science's (SC) EXAALT project while Kokkos received funding through both SC and the National Nuclear Security Administration (NNSA).  NNSA mission-related codes and open science codes across the DOE scientific landscape leverage Kokkos' core abstractions targeting parallel execution of code and its data management capabilities. Increasing usage of Kokkos by focusing on both SC scientific applications and NNSA applications promotes the sustainability of Kokkos through a broader user and developer base. A broader user base, for example, provides incentive to develop more extensive documentation or a greater web presence such as demonstrated with Kokkos' homepage, www.kokkos.org. 
The long-term sustained funding of Kokkos to support NNSA mission work signals to the external community that they can trust Kokkos that Kokkos will be supported for years to come, making it easier for teams to adopt Kokkos and for SC to trust that sponsorship of Kokkos-related projects will be part of a robust ecosystem.  Multiple funding sources for Kokkos improves sustainability and resilience to funding changes.  This robust funding and the expectation to support external users provide resources and motivation that enable a user-focused mindset and encourages projects to pursue outward facing indicators of sustainability such as Kokkos' recent OpenSSF Best Practices *Passing* badge.

## Additional resources

**LAMMPS citations:**

LAMMPS - a flexible simulation tool for particle-based materials modeling at the atomic, meso, and continuum scales, A. P. Thompson, H. M. Aktulga, R. Berger, D. S. Bolintineanu, W. M. Brown, P. S. Crozier, P. J. in 't Veld, A. Kohlmeyer, S. G. Moore, T. D. Nguyen, R. Shan, M. J. Stevens, J. Tranchida, C. Trott, S. J. Plimpton, Comp Phys Comm, 271 (2022) 10817.  <https://doi.org/10.1016/j.cpc.2021.108171>.

K. Nguyen-Cong, J. T. Willman, S. G. Moore, A. B. Belonoshko, R. Gayatri, E. Weinberg, M. A. Wood, A. P. Thompson, and I I. Oleynik. 2021. Billion atom molecular dynamics simulations of carbon at extreme conditions and experimental time and length scales. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (SC '21). Association for Computing Machinery, New York, NY, USA, Article 4, 1â€“12. <https://doi.org/10.1145/3458817.3487400>.

